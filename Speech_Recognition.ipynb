{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Speech Recognition\n",
    "\n",
    "Since maxout networks seem to be popular for speech recognition tasks, we decided to run some experiments testing our initialization strategy on the `Speech Commands` dataset. In this notebook we explore how to use it in scope of `Pytorch` package. We follow an official `Pytorch` `Speech Commands` [tutorial](https://pytorch.org/tutorials/intermediate/speech_command_classification_with_torchaudio_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#import torchaudio\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from tqdm import tqdm\n",
    "#import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "#import torchaudio.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Note: we encountered issues with incompatible versions of pytorch and torchaudio packages in conda environment. Running the following commands ensured the installed versions were compatible:*\n",
    "\n",
    "    conda install pytorch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0 cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubsetSC(SPEECHCOMMANDS):\n",
    "    def __init__(self, subset: str = None):\n",
    "        super().__init__(\"./\", download=True)\n",
    "\n",
    "        def load_list(filename):\n",
    "            filepath = os.path.join(self._path, filename)\n",
    "            with open(filepath) as fileobj:\n",
    "                return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj]\n",
    "\n",
    "        if subset == \"validation\":\n",
    "            self._walker = load_list(\"validation_list.txt\")\n",
    "        elif subset == \"testing\":\n",
    "            self._walker = load_list(\"testing_list.txt\")\n",
    "        elif subset == \"training\":\n",
    "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
    "            excludes = set(excludes)\n",
    "            self._walker = [w for w in self._walker if w not in excludes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing split of the data. We do not use validation in this tutorial.\n",
    "train_set = SubsetSC(\"training\")\n",
    "test_set = SubsetSC(\"testing\")\n",
    "\n",
    "waveform, sample_rate, label, speaker_id, utterance_number = train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(train_set[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes --> tensor(33) --> yes\n"
     ]
    }
   ],
   "source": [
    "Labels = sorted(list(set(datapoint[2] for datapoint in train_set)))\n",
    "\n",
    "def label_to_index(word):\n",
    "    # Return the position of the word in labels\n",
    "    return torch.tensor(Labels.index(word))\n",
    "\n",
    "\n",
    "def index_to_label(index):\n",
    "    # Return the word corresponding to the index in labels\n",
    "    # This is the inverse of label_to_index\n",
    "    return Labels[index]\n",
    "\n",
    "\n",
    "word_start = \"yes\"\n",
    "index = label_to_index(word_start)\n",
    "word_recovered = index_to_label(index)\n",
    "\n",
    "print(word_start, \"-->\", index, \"-->\", word_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(batch):\n",
    "    # Make all tensor in a batch the same length by padding with zeros\n",
    "    batch = [item.t() for item in batch]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
    "    return batch.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    # A data tuple has the form:\n",
    "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
    "\n",
    "    tensors, targets = [], []\n",
    "\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for waveform, _, label, *_ in batch:\n",
    "        tensors += [waveform]\n",
    "        targets += [label_to_index(label)]\n",
    "\n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = pad_sequence(tensors)\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    return tensors, targets\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "if device == \"cuda\":\n",
    "    num_workers = 1\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the network\n",
    "\n",
    "Here we will deviate from the official `Speech Commands` tutorial. We will define two network types: ReLU DNN and Maxout DNN and try to compare the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maxout hyperparameters:\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "learning_rate = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxoutNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaxoutNet, self).__init__()\n",
    "        self.lay1lin1 = nn.Linear(N, 128)\n",
    "        self.lay1lin2 = nn.Linear(N, 128)\n",
    "        self.lay1lin3 = nn.Linear(N, 128)\n",
    "        self.lay2lin1 = nn.Linear(128, 35)\n",
    "        self.lay2lin2 = nn.Linear(128, 35)\n",
    "        self.lay2lin3 = nn.Linear(128, 35)\n",
    "        self.maxout_rank = 3\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) # I do not understand this black magic, x is not a single image, but a tensor of images. How does this code work?\n",
    "        x = x.unsqueeze(0) # make vector of length 784 into 1*784 matrix\n",
    "        X = torch.cat( (self.lay1lin1(x),self.lay1lin2(x),self.lay1lin3(x)), 0)\n",
    "              # concatenate output vectors into matrix (row-wise by default)\n",
    "              # size: rank * width layer 1\n",
    "        x,dummy = torch.max(X,0)\n",
    "              # go through each column and compute max\n",
    "              # size: 1 * width layer 1\n",
    "        x = x.unsqueeze(0)\n",
    "        X = torch.cat( (self.lay2lin1(x),self.lay2lin2(x),self.lay2lin3(x)), 0)\n",
    "              # concatenate output vectors into matrix (row-wise by default)\n",
    "              # size: rank * width layer 2\n",
    "        x,dummy = torch.max(X,0)\n",
    "              # go through each column and compute max\n",
    "              # size: 1 * width layer 2\n",
    "        # x = mySoftmax(x) # wth does this make loss worse?\n",
    "        return x\n",
    "\n",
    "model_maxout = MaxoutNet().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_relu = nn.ReLU()\n",
    "output_normalisation = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "class ReLUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReLUNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(N, 1024)\n",
    "        #self.layer2 = nn.Linear(10, 10)\n",
    "        self.layer3 = nn.Linear(1024, 35)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) # I do not understand this black magic, x is not a single image, but a tensor of images. How does this code work?\n",
    "        x = activation_relu(self.layer1(x))\n",
    "        x# = activation(self.layer2(x))\n",
    "        x = output_normalisation(self.layer3(x))\n",
    "        return x\n",
    "\n",
    "model_relu = ReLUNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion_maxout = nn.CrossEntropyLoss()\n",
    "optimizer_maxout = optim.Adam(model_maxout.parameters(), lr=learning_rate)#, weight_decay=0.0001)\n",
    "\n",
    "#optimizer_maxout = torch.optim.SGD(model_maxout.parameters(), lr=learning_rate)#optimizer_maxout = #optim.Adam(model_maxout.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "#scheduler_maxout = #optim.lr_scheduler.StepLR(optimizer_maxout, step_size=20, gamma=0.1)  # reduce the learning after 20 epochs by a factor of 10\n",
    "optimizer_relu = optim.Adam(model_relu.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "scheduler_relu = optim.lr_scheduler.StepLR(optimizer_relu, step_size=20, gamma=0.1)  # reduce the learning after 20 epochs by a factor of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_maxout(model, epoch, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        #data = transform(data)\n",
    "        output = model(data)\n",
    "\n",
    "        # negative log-likelihood for a tensor of size (batch x 1 x n_output)\n",
    "        #criterion = nn.CrossEntropyLoss()#F.nll_loss(output.squeeze(), target)\n",
    "        #loss = criterion_maxout(output.squeeze(), target)\n",
    "        loss = F.nll_loss(output.squeeze(), target)\n",
    "\n",
    "        optimizer_maxout.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_maxout.step()\n",
    "\n",
    "        # print training stats\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "        # record loss\n",
    "        losses_maxout.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_maxout = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_correct(pred, target):\n",
    "    # count number of correct predictions\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "\n",
    "def get_likely_index(tensor):\n",
    "    # find most likely label index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)\n",
    "\n",
    "\n",
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        #data = transform(data)\n",
    "        output = model(data)\n",
    "\n",
    "        pred = get_likely_index(output)\n",
    "        correct += number_of_correct(pred, target)\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "\n",
    "    print(f\"\\nTest Epoch: {epoch}\\tAccuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_interval = 20\n",
    "# n_epoch = 2\n",
    "\n",
    "# pbar_update = 1 / (len(train_loader) + len(test_loader))\n",
    "# #losses = []\n",
    "\n",
    "# # The transform needs to live on the same device as the model and the data.\n",
    "# #transform = transform.to(device)\n",
    "# with tqdm(total=n_epoch) as pbar:\n",
    "#     for epoch in range(1, n_epoch + 1):\n",
    "#         train_maxout(model_maxout, epoch, log_interval)\n",
    "#         test(model_maxout, epoch)\n",
    "#         scheduler_maxout.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [50/332], Loss: -6765002752.0000\n",
      "Epoch [1/5], Step [100/332], Loss: -7062084096.0000\n",
      "Epoch [1/5], Step [150/332], Loss: -8598843392.0000\n",
      "Epoch [1/5], Step [200/332], Loss: -6564698112.0000\n",
      "Epoch [1/5], Step [250/332], Loss: -8970320896.0000\n",
      "Epoch [1/5], Step [300/332], Loss: -7301060096.0000\n",
      "Epoch [2/5], Step [50/332], Loss: -10095347712.0000\n",
      "Epoch [2/5], Step [100/332], Loss: -9870902272.0000\n",
      "Epoch [2/5], Step [150/332], Loss: -10928656384.0000\n",
      "Epoch [2/5], Step [200/332], Loss: -10719845376.0000\n",
      "Epoch [2/5], Step [250/332], Loss: -11512903680.0000\n",
      "Epoch [2/5], Step [300/332], Loss: -11539314688.0000\n",
      "Epoch [3/5], Step [50/332], Loss: -10926978048.0000\n",
      "Epoch [3/5], Step [100/332], Loss: -12152360960.0000\n",
      "Epoch [3/5], Step [150/332], Loss: -11847414784.0000\n",
      "Epoch [3/5], Step [200/332], Loss: -12777988096.0000\n",
      "Epoch [3/5], Step [250/332], Loss: -13549804544.0000\n",
      "Epoch [3/5], Step [300/332], Loss: -14111321088.0000\n",
      "Epoch [4/5], Step [50/332], Loss: -14587561984.0000\n",
      "Epoch [4/5], Step [100/332], Loss: -16288043008.0000\n",
      "Epoch [4/5], Step [150/332], Loss: -14576339968.0000\n",
      "Epoch [4/5], Step [200/332], Loss: -16075123712.0000\n",
      "Epoch [4/5], Step [250/332], Loss: -16769385472.0000\n",
      "Epoch [4/5], Step [300/332], Loss: -22061965312.0000\n",
      "Epoch [5/5], Step [50/332], Loss: -17552398336.0000\n",
      "Epoch [5/5], Step [100/332], Loss: -18465802240.0000\n",
      "Epoch [5/5], Step [150/332], Loss: -16702188544.0000\n",
      "Epoch [5/5], Step [200/332], Loss: -19564933120.0000\n",
      "Epoch [5/5], Step [250/332], Loss: -19027259392.0000\n",
      "Epoch [5/5], Step [300/332], Loss: -22373869568.0000\n",
      "Finished Training\n",
      "Accuracy of the network: 3.6528850522489775 %\n",
      "Accuracy of backward: 0.0 %\n",
      "Accuracy of bed: 0.0 %\n",
      "Accuracy of bird: 0.0 %\n",
      "Accuracy of cat: 0.0 %\n",
      "Accuracy of dog: 0.0 %\n",
      "Accuracy of down: 0.0 %\n",
      "Accuracy of eight: 0.0 %\n",
      "Accuracy of five: 0.0 %\n",
      "Accuracy of follow: 0.0 %\n",
      "Accuracy of forward: 0.0 %\n",
      "Accuracy of four: 0.0 %\n",
      "Accuracy of go: 100.0 %\n",
      "Accuracy of happy: 0.0 %\n",
      "Accuracy of house: 0.0 %\n",
      "Accuracy of learn: 0.0 %\n",
      "Accuracy of left: 0.0 %\n",
      "Accuracy of marvin: 0.0 %\n",
      "Accuracy of nine: 0.0 %\n",
      "Accuracy of no: 0.0 %\n",
      "Accuracy of off: 0.0 %\n",
      "Accuracy of on: 0.0 %\n",
      "Accuracy of one: 0.0 %\n",
      "Accuracy of right: 0.0 %\n",
      "Accuracy of seven: 0.0 %\n",
      "Accuracy of sheila: 0.0 %\n",
      "Accuracy of six: 0.0 %\n",
      "Accuracy of stop: 0.0 %\n",
      "Accuracy of three: 0.0 %\n",
      "Accuracy of tree: 0.0 %\n",
      "Accuracy of two: 0.0 %\n",
      "Accuracy of up: 0.0 %\n",
      "Accuracy of visual: 0.0 %\n",
      "Accuracy of wow: 0.0 %\n",
      "Accuracy of yes: 0.0 %\n",
      "Accuracy of zero: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        #print(i)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_maxout(images)\n",
    "        loss = F.nll_loss(outputs.squeeze(), labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer_maxout.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_maxout.step()\n",
    "\n",
    "        if (i+1) % 50 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Finished Training')\n",
    "# PATH = './maxoutMNIST.pth'\n",
    "# torch.save(model.state_dict(), PATH)\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(35)]\n",
    "    n_class_samples = [0 for i in range(35)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_maxout(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(35):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {Labels[i]}: {acc} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
